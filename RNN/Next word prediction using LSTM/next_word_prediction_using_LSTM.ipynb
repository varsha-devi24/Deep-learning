{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9185ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d0544f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of Alice in Wonderland, by Lewis Carroll This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org Title: Alice in Wonderland Author: Lewis Carroll Illustrator: Gordon Robinson Release Date: August 12, 2006 [EBook #19033] Language: English *** START OF THIS PROJECT GUTENBERG EBOOK '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"alice_in_wonderland.txt\", \"r\", encoding = \"utf8\")\n",
    "\n",
    "# store file in list\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "# Convert list to string\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    "\n",
    "#replace unnecessary stuff with space\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
    "\n",
    "#remove unnecessary spaces \n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4af7e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71670"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "392e15fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 22, 21, 180, 5, 10, 7, 277, 37, 554, 555, 19, 180, 26, 25]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c816005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13074"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74f31601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73ab4bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  13071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1,  22,  21, 180],\n",
       "       [ 22,  21, 180,   5],\n",
       "       [ 21, 180,   5,  10],\n",
       "       [180,   5,  10,   7],\n",
       "       [  5,  10,   7, 277],\n",
       "       [ 10,   7, 277,  37],\n",
       "       [  7, 277,  37, 554],\n",
       "       [277,  37, 554, 555],\n",
       "       [ 37, 554, 555,  19],\n",
       "       [554, 555,  19, 180]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a46e1b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9925d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[  1  22  21]\n",
      " [ 22  21 180]\n",
      " [ 21 180   5]\n",
      " [180   5  10]\n",
      " [  5  10   7]\n",
      " [ 10   7 277]\n",
      " [  7 277  37]\n",
      " [277  37 554]\n",
      " [ 37 554 555]\n",
      " [554 555  19]]\n",
      "Response:  [180   5  10   7 277  37 554 555  19 180]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "845ebc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f439e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9195e2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 3, 10)             20800     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 3, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2080)              2082080   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,151,880\n",
      "Trainable params: 15,151,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df035ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 6.4588\n",
      "Epoch 1: loss improved from inf to 6.45878, saving model to next_words.h5\n",
      "205/205 [==============================] - 294s 1s/step - loss: 6.4588\n",
      "Epoch 2/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 6.1160\n",
      "Epoch 2: loss improved from 6.45878 to 6.11604, saving model to next_words.h5\n",
      "205/205 [==============================] - 246s 1s/step - loss: 6.1160\n",
      "Epoch 3/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 5.8152\n",
      "Epoch 3: loss improved from 6.11604 to 5.81521, saving model to next_words.h5\n",
      "205/205 [==============================] - 249s 1s/step - loss: 5.8152\n",
      "Epoch 4/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 5.4975\n",
      "Epoch 4: loss improved from 5.81521 to 5.49749, saving model to next_words.h5\n",
      "205/205 [==============================] - 257s 1s/step - loss: 5.4975\n",
      "Epoch 5/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 5.2486\n",
      "Epoch 5: loss improved from 5.49749 to 5.24859, saving model to next_words.h5\n",
      "205/205 [==============================] - 252s 1s/step - loss: 5.2486\n",
      "Epoch 6/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 5.0244\n",
      "Epoch 6: loss improved from 5.24859 to 5.02443, saving model to next_words.h5\n",
      "205/205 [==============================] - 307s 1s/step - loss: 5.0244\n",
      "Epoch 7/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 4.8218\n",
      "Epoch 7: loss improved from 5.02443 to 4.82184, saving model to next_words.h5\n",
      "205/205 [==============================] - 316s 2s/step - loss: 4.8218\n",
      "Epoch 8/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 4.6114\n",
      "Epoch 8: loss improved from 4.82184 to 4.61145, saving model to next_words.h5\n",
      "205/205 [==============================] - 270s 1s/step - loss: 4.6114\n",
      "Epoch 9/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 4.4127\n",
      "Epoch 9: loss improved from 4.61145 to 4.41268, saving model to next_words.h5\n",
      "205/205 [==============================] - 250s 1s/step - loss: 4.4127\n",
      "Epoch 10/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 4.2056\n",
      "Epoch 10: loss improved from 4.41268 to 4.20562, saving model to next_words.h5\n",
      "205/205 [==============================] - 245s 1s/step - loss: 4.2056\n",
      "Epoch 11/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 3.9864\n",
      "Epoch 11: loss improved from 4.20562 to 3.98642, saving model to next_words.h5\n",
      "205/205 [==============================] - 241s 1s/step - loss: 3.9864\n",
      "Epoch 12/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 3.7594\n",
      "Epoch 12: loss improved from 3.98642 to 3.75936, saving model to next_words.h5\n",
      "205/205 [==============================] - 231s 1s/step - loss: 3.7594\n",
      "Epoch 13/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 3.5016\n",
      "Epoch 13: loss improved from 3.75936 to 3.50159, saving model to next_words.h5\n",
      "205/205 [==============================] - 227s 1s/step - loss: 3.5016\n",
      "Epoch 14/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 3.2294\n",
      "Epoch 14: loss improved from 3.50159 to 3.22942, saving model to next_words.h5\n",
      "205/205 [==============================] - 236s 1s/step - loss: 3.2294\n",
      "Epoch 15/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 2.9605\n",
      "Epoch 15: loss improved from 3.22942 to 2.96047, saving model to next_words.h5\n",
      "205/205 [==============================] - 240s 1s/step - loss: 2.9605\n",
      "Epoch 16/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 2.6962\n",
      "Epoch 16: loss improved from 2.96047 to 2.69625, saving model to next_words.h5\n",
      "205/205 [==============================] - 226s 1s/step - loss: 2.6962\n",
      "Epoch 17/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 2.4362\n",
      "Epoch 17: loss improved from 2.69625 to 2.43618, saving model to next_words.h5\n",
      "205/205 [==============================] - 231s 1s/step - loss: 2.4362\n",
      "Epoch 18/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 2.1752\n",
      "Epoch 18: loss improved from 2.43618 to 2.17524, saving model to next_words.h5\n",
      "205/205 [==============================] - 248s 1s/step - loss: 2.1752\n",
      "Epoch 19/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.9479\n",
      "Epoch 19: loss improved from 2.17524 to 1.94789, saving model to next_words.h5\n",
      "205/205 [==============================] - 234s 1s/step - loss: 1.9479\n",
      "Epoch 20/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.7244\n",
      "Epoch 20: loss improved from 1.94789 to 1.72440, saving model to next_words.h5\n",
      "205/205 [==============================] - 232s 1s/step - loss: 1.7244\n",
      "Epoch 21/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.5333\n",
      "Epoch 21: loss improved from 1.72440 to 1.53328, saving model to next_words.h5\n",
      "205/205 [==============================] - 223s 1s/step - loss: 1.5333\n",
      "Epoch 22/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.3443\n",
      "Epoch 22: loss improved from 1.53328 to 1.34435, saving model to next_words.h5\n",
      "205/205 [==============================] - 227s 1s/step - loss: 1.3443\n",
      "Epoch 23/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.1828\n",
      "Epoch 23: loss improved from 1.34435 to 1.18277, saving model to next_words.h5\n",
      "205/205 [==============================] - 228s 1s/step - loss: 1.1828\n",
      "Epoch 24/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.0369\n",
      "Epoch 24: loss improved from 1.18277 to 1.03687, saving model to next_words.h5\n",
      "205/205 [==============================] - 219s 1s/step - loss: 1.0369\n",
      "Epoch 25/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.9229\n",
      "Epoch 25: loss improved from 1.03687 to 0.92286, saving model to next_words.h5\n",
      "205/205 [==============================] - 223s 1s/step - loss: 0.9229\n",
      "Epoch 26/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.8140 \n",
      "Epoch 26: loss improved from 0.92286 to 0.81397, saving model to next_words.h5\n",
      "205/205 [==============================] - 4383s 21s/step - loss: 0.8140\n",
      "Epoch 27/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.7109\n",
      "Epoch 27: loss improved from 0.81397 to 0.71094, saving model to next_words.h5\n",
      "205/205 [==============================] - 233s 1s/step - loss: 0.7109\n",
      "Epoch 28/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.6538\n",
      "Epoch 28: loss improved from 0.71094 to 0.65376, saving model to next_words.h5\n",
      "205/205 [==============================] - 225s 1s/step - loss: 0.6538\n",
      "Epoch 29/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.5967\n",
      "Epoch 29: loss improved from 0.65376 to 0.59669, saving model to next_words.h5\n",
      "205/205 [==============================] - 229s 1s/step - loss: 0.5967\n",
      "Epoch 30/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.5332\n",
      "Epoch 30: loss improved from 0.59669 to 0.53323, saving model to next_words.h5\n",
      "205/205 [==============================] - 223s 1s/step - loss: 0.5332\n",
      "Epoch 31/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.4938\n",
      "Epoch 31: loss improved from 0.53323 to 0.49383, saving model to next_words.h5\n",
      "205/205 [==============================] - 227s 1s/step - loss: 0.4938\n",
      "Epoch 32/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.4679\n",
      "Epoch 32: loss improved from 0.49383 to 0.46786, saving model to next_words.h5\n",
      "205/205 [==============================] - 222s 1s/step - loss: 0.4679\n",
      "Epoch 33/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.4706\n",
      "Epoch 33: loss did not improve from 0.46786\n",
      "205/205 [==============================] - 225s 1s/step - loss: 0.4706\n",
      "Epoch 34/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.4204\n",
      "Epoch 34: loss improved from 0.46786 to 0.42040, saving model to next_words.h5\n",
      "205/205 [==============================] - 221s 1s/step - loss: 0.4204\n",
      "Epoch 35/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.4205\n",
      "Epoch 35: loss did not improve from 0.42040\n",
      "205/205 [==============================] - 226s 1s/step - loss: 0.4205\n",
      "Epoch 36/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3855\n",
      "Epoch 36: loss improved from 0.42040 to 0.38547, saving model to next_words.h5\n",
      "205/205 [==============================] - 1559s 8s/step - loss: 0.3855\n",
      "Epoch 37/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3724\n",
      "Epoch 37: loss improved from 0.38547 to 0.37240, saving model to next_words.h5\n",
      "205/205 [==============================] - 234s 1s/step - loss: 0.3724\n",
      "Epoch 38/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3661\n",
      "Epoch 38: loss improved from 0.37240 to 0.36614, saving model to next_words.h5\n",
      "205/205 [==============================] - 230s 1s/step - loss: 0.3661\n",
      "Epoch 39/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3491\n",
      "Epoch 39: loss improved from 0.36614 to 0.34911, saving model to next_words.h5\n",
      "205/205 [==============================] - 224s 1s/step - loss: 0.3491\n",
      "Epoch 40/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3369\n",
      "Epoch 40: loss improved from 0.34911 to 0.33692, saving model to next_words.h5\n",
      "205/205 [==============================] - 224s 1s/step - loss: 0.3369\n",
      "Epoch 41/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3385\n",
      "Epoch 41: loss did not improve from 0.33692\n",
      "205/205 [==============================] - 220s 1s/step - loss: 0.3385\n",
      "Epoch 42/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3381\n",
      "Epoch 42: loss did not improve from 0.33692\n",
      "205/205 [==============================] - 224s 1s/step - loss: 0.3381\n",
      "Epoch 43/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3255\n",
      "Epoch 43: loss improved from 0.33692 to 0.32550, saving model to next_words.h5\n",
      "205/205 [==============================] - 221s 1s/step - loss: 0.3255\n",
      "Epoch 44/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3195\n",
      "Epoch 44: loss improved from 0.32550 to 0.31952, saving model to next_words.h5\n",
      "205/205 [==============================] - 228s 1s/step - loss: 0.3195\n",
      "Epoch 45/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3237\n",
      "Epoch 45: loss did not improve from 0.31952\n",
      "205/205 [==============================] - 217s 1s/step - loss: 0.3237\n",
      "Epoch 46/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3240 \n",
      "Epoch 46: loss did not improve from 0.31952\n",
      "205/205 [==============================] - 4832s 24s/step - loss: 0.3240\n",
      "Epoch 47/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3173\n",
      "Epoch 47: loss improved from 0.31952 to 0.31731, saving model to next_words.h5\n",
      "205/205 [==============================] - 230s 1s/step - loss: 0.3173\n",
      "Epoch 48/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3254\n",
      "Epoch 48: loss did not improve from 0.31731\n",
      "205/205 [==============================] - 232s 1s/step - loss: 0.3254\n",
      "Epoch 49/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3137\n",
      "Epoch 49: loss improved from 0.31731 to 0.31370, saving model to next_words.h5\n",
      "205/205 [==============================] - 231s 1s/step - loss: 0.3137\n",
      "Epoch 50/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3072\n",
      "Epoch 50: loss improved from 0.31370 to 0.30719, saving model to next_words.h5\n",
      "205/205 [==============================] - 234s 1s/step - loss: 0.3072\n",
      "Epoch 51/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3049\n",
      "Epoch 51: loss improved from 0.30719 to 0.30494, saving model to next_words.h5\n",
      "205/205 [==============================] - 227s 1s/step - loss: 0.3049\n",
      "Epoch 52/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2849\n",
      "Epoch 52: loss improved from 0.30494 to 0.28488, saving model to next_words.h5\n",
      "205/205 [==============================] - 224s 1s/step - loss: 0.2849\n",
      "Epoch 53/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2804\n",
      "Epoch 53: loss improved from 0.28488 to 0.28036, saving model to next_words.h5\n",
      "205/205 [==============================] - 223s 1s/step - loss: 0.2804\n",
      "Epoch 54/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2881\n",
      "Epoch 54: loss did not improve from 0.28036\n",
      "205/205 [==============================] - 309s 2s/step - loss: 0.2881\n",
      "Epoch 55/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2738\n",
      "Epoch 55: loss improved from 0.28036 to 0.27385, saving model to next_words.h5\n",
      "205/205 [==============================] - 230s 1s/step - loss: 0.2738\n",
      "Epoch 56/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2738\n",
      "Epoch 56: loss improved from 0.27385 to 0.27375, saving model to next_words.h5\n",
      "205/205 [==============================] - 224s 1s/step - loss: 0.2738\n",
      "Epoch 57/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2609\n",
      "Epoch 57: loss improved from 0.27375 to 0.26089, saving model to next_words.h5\n",
      "205/205 [==============================] - 231s 1s/step - loss: 0.2609\n",
      "Epoch 58/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2532\n",
      "Epoch 58: loss improved from 0.26089 to 0.25318, saving model to next_words.h5\n",
      "205/205 [==============================] - 224s 1s/step - loss: 0.2532\n",
      "Epoch 59/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2523\n",
      "Epoch 59: loss improved from 0.25318 to 0.25228, saving model to next_words.h5\n",
      "205/205 [==============================] - 231s 1s/step - loss: 0.2523\n",
      "Epoch 60/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2644\n",
      "Epoch 60: loss did not improve from 0.25228\n",
      "205/205 [==============================] - 222s 1s/step - loss: 0.2644\n",
      "Epoch 61/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2686\n",
      "Epoch 61: loss did not improve from 0.25228\n",
      "205/205 [==============================] - 222s 1s/step - loss: 0.2686\n",
      "Epoch 62/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2823\n",
      "Epoch 62: loss did not improve from 0.25228\n",
      "205/205 [==============================] - 218s 1s/step - loss: 0.2823\n",
      "Epoch 63/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2727\n",
      "Epoch 63: loss did not improve from 0.25228\n",
      "205/205 [==============================] - 609s 3s/step - loss: 0.2727\n",
      "Epoch 64/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2671\n",
      "Epoch 64: loss did not improve from 0.25228\n",
      "205/205 [==============================] - 225s 1s/step - loss: 0.2671\n",
      "Epoch 65/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2559\n",
      "Epoch 65: loss did not improve from 0.25228\n",
      "205/205 [==============================] - 226s 1s/step - loss: 0.2559\n",
      "Epoch 66/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2579\n",
      "Epoch 66: loss did not improve from 0.25228\n",
      "205/205 [==============================] - 226s 1s/step - loss: 0.2579\n",
      "Epoch 67/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2741\n",
      "Epoch 67: loss did not improve from 0.25228\n",
      "205/205 [==============================] - 228s 1s/step - loss: 0.2741\n",
      "Epoch 68/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2678\n",
      "Epoch 68: loss did not improve from 0.25228\n",
      "205/205 [==============================] - 224s 1s/step - loss: 0.2678\n",
      "Epoch 69/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2469\n",
      "Epoch 69: loss improved from 0.25228 to 0.24687, saving model to next_words.h5\n",
      "205/205 [==============================] - 226s 1s/step - loss: 0.2469\n",
      "Epoch 70/70\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2327\n",
      "Epoch 70: loss improved from 0.24687 to 0.23270, saving model to next_words.h5\n",
      "205/205 [==============================] - 226s 1s/step - loss: 0.2327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b8d8518b20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d29c18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "\n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  sequence = np.array(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "  \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "  \n",
    "  print(predicted_word)\n",
    "  return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4979fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line:  Alice was beginning\n",
      "['Alice', 'was', 'beginning']\n",
      "to\n",
      "Enter your line: no pictures or \n",
      "['pictures', 'or', '']\n",
      "Error occurred:  in user code:\n",
      "\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n",
      "        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n",
      "\n",
      "    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 2)\n",
      "\n",
      "Enter your line: Down, down, down!\n",
      "['Down,', 'down,', 'down!']\n",
      "Error occurred:  in user code:\n",
      "\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n",
      "        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n",
      "\n",
      "    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 3), found shape=(None, 0)\n",
      "\n",
      "Enter your line: Alice felt that\n",
      "['Alice', 'felt', 'that']\n",
      "she\n",
      "Enter your line: 0\n",
      "Execution completed.....\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "  text = input(\"Enter your line: \")\n",
    "  \n",
    "  if text == \"0\":\n",
    "      print(\"Execution completed.....\")\n",
    "      break\n",
    "  \n",
    "  else:\n",
    "      try:\n",
    "          text = text.split(\" \")\n",
    "          text = text[-3:]\n",
    "          print(text)\n",
    "        \n",
    "          Predict_Next_Words(model, tokenizer, text)\n",
    "          \n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd4293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
